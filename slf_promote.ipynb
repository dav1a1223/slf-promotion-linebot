{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import jieba as jb\n",
    "import pickle\n",
    "from gensim.models import word2vec\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = pickle.load(open(\"chat.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/9t/syxn77t51vnbj9ntx09wpzlr0000gn/T/jieba.cache\n",
      "Loading model cost 1.224 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "flat_list = [jb.lcut(item) for sublist in chats for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmodel = word2vec.Word2Vec(flat_list, size=250, sg=1)\n",
    "wmodel.save(\"wmodel.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\" \".join(item) for item in flat_list]\n",
    "vectorizer = CountVectorizer(max_features=7000,token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names()\n",
    "counts = X.toarray()\n",
    "# word = vectorizer.get_feature_names()  \n",
    "# 获取每个词在该行（文档）中出现的次数\n",
    "counts =  X.toarray()\n",
    "transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "# tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open(\"vectorizer\", \"wb\"))\n",
    "pickle.dump(transformer, open(\"tfidf\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pickle.load(open(\"vectorizer\", \"rb\"))\n",
    "transformer = pickle.load(open(\"tfidf\", \"rb\"))\n",
    "wmodel = word2vec.Word2Vec.load(\"wmodel.w2v\")\n",
    "query = \"要不要跟我約會？\"\n",
    "candidate_replies = {\n",
    "    \"興趣，喜好\": \"我的興趣是打羽毛球與打爵士鼓\",\n",
    "    \"專長，擅長\": \"我的專長是打羽毛球，從高中開始打了9年\",\n",
    "    \"程式語言，寫code\": \"我比較擅長Python與Ruby, 擅長的深度學習工具是PyTorch\",\n",
    "    \"學歷，大學，研究所\": \"我大學念台大資管，目前繼續就讀台大資管所碩二\",\n",
    "    \"論文，研究\": \"我的碩士論文主要研究Machine Reading Comprehension的技術\",\n",
    "    \"工作經驗，實習\": \"大學時曾經在iCook實習一年擔任後端工程師, 碩一暑假在台達電擔任NLP工程師\",\n",
    "    \"工作內容\": \"iCook時設計與建置API, 台達電時研究前沿的自動摘要技術\",\n",
    "    \"自我介紹\": \"目前就讀台灣大學資訊管理學系碩二，研究所期間主要在研究Deep Learning NLP領域中的閱讀理解問題，實際實作過許多論文中的模型，並正在撰寫碩士論文目標投稿ACL 2019。工作經驗有在台達電子做過自動摘要技術的研究，以及大學部期間擁有一年的後端開發實習經驗，熟悉設計與實作基礎的網頁系統與API。\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sent):\n",
    "    sent_split = jb.lcut(sent)\n",
    "    sent_blank = \" \".join(sent_split)\n",
    "    sent_tfidf = transformer.transform(vectorizer.transform([sent_blank])).toarray()[0]\n",
    "    sent_vec = []\n",
    "    for word in sent_split:\n",
    "        if word in wmodel.wv.vocab:\n",
    "            if word in vectorizer.vocabulary_:\n",
    "                weight = sent_tfidf[vectorizer.vocabulary_[word]]\n",
    "            else:\n",
    "                weight = 0.05\n",
    "            if len(sent_vec) == 0:\n",
    "                sent_vec = weight * wmodel.wv[word]\n",
    "            else:\n",
    "                sent_vec += weight * wmodel.wv[word]\n",
    "    return sent_vec / len(sent_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "興趣，喜好 : 0.34394213557243347\n",
      "專長，擅長 : 0.39824777841567993\n",
      "程式語言，寫code : 0.36028310656547546\n",
      "學歷，大學，研究所 : 0.3890741169452667\n",
      "論文，研究 : 0.3212156295776367\n",
      "工作經驗，實習 : 0.4011821150779724\n",
      "工作內容 : 0.3849673271179199\n",
      "自我介紹 : 0.3878204822540283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'大學時曾經在iCook實習一年擔任後端工程師, 碩一暑假在台達電擔任NLP工程師'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vec = sent2vec(query)\n",
    "max_similarity = -1\n",
    "thres = 0.0\n",
    "cur_cand = \"抱歉我不懂您的意思\"\n",
    "for cand in candidate_replies.keys():\n",
    "    print(cand, \": \", end=\"\")\n",
    "    cand_vec = sent2vec(cand)\n",
    "#     simi = np.dot(query_vec, cand_vec)\n",
    "    simi = 1 - spatial.distance.cosine(query_vec, cand_vec)\n",
    "    print(simi)\n",
    "    if simi > max_similarity and simi > thres:\n",
    "        max_similarity = simi\n",
    "        cur_cand = cand\n",
    "candidate_replies[cur_cand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
