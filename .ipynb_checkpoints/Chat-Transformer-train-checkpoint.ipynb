{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import jieba as jb\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import pdb\n",
    "from opencc import OpenCC \n",
    "embedding_size = 256\n",
    "embed_type = \"word\" # \"char\"\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\I-DAVI~1.LAI\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.950 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# def full2half(s):\n",
    "#     n = []\n",
    "# #     if isinstance(s, str):\n",
    "# #         s = s.decode('utf-8')\n",
    "#     for char in s:\n",
    "#         num = ord(char)\n",
    "#         if num == 0x3000:\n",
    "#             num = 32\n",
    "#         elif 0xFF01 <= num <= 0xFF5E:\n",
    "#             num -= 0xfee0\n",
    "#         num = chr(num)\n",
    "#         n.append(num)\n",
    "#     return ''.join(n)\n",
    "# with open(\"Gossiping-QA-Dataset.txt\", \"r\", encoding='utf8') as f:\n",
    "#     content = f.readlines()\n",
    "# qs = []\n",
    "# anss = []\n",
    "# for cont in content:\n",
    "#     q, ans = cont.split(\"\\t\")\n",
    "#     q = full2half(q.replace(\" \", \"\").strip())\n",
    "#     ans = full2half(ans.replace(\" \", \"\").strip())\n",
    "#     if embed_type == \"word\":\n",
    "#         q = jb.lcut(q)\n",
    "#         ans = jb.lcut(ans)\n",
    "#     elif embed_type == \"char\":\n",
    "#         q = list(q)\n",
    "#         ans = list(ans)\n",
    "#     qs.append(q)\n",
    "#     anss.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51727\n"
     ]
    }
   ],
   "source": [
    "# w2v = Word2Vec(qs+anss, size=embedding_size, min_count=5)\n",
    "# print(len(w2v.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2ix = {\"<BOS>\": 0, \"<EOS>\": 1, \"<PAD>\": 2, \"<UNK>\": 3}\n",
    "# ix2word = {0: \"<BOS>\", 1: \"<EOS>\", 2: \"<PAD>\", 3: \"<UNK>\"}\n",
    "# next_ix = 4\n",
    "# all_sent = qs + anss\n",
    "# for i, sent in enumerate(all_sent):\n",
    "#     for j, word in enumerate(sent):\n",
    "#         if word in w2v.wv.vocab and word not in word2ix:\n",
    "#             word2ix[word] = next_ix\n",
    "#             ix2word[next_ix] = word\n",
    "#             next_ix += 1\n",
    "#         elif word not in w2v.wv.vocab:\n",
    "#             all_sent[i][j] = \"<UNK>\"\n",
    "# qs, anss = all_sent[:int(len(all_sent)/2)], all_sent[int(len(all_sent)/2):]\n",
    "# del all_sent\n",
    "# weights = []\n",
    "# for ix, word in sorted(ix2word.items(), key=lambda x: x[0]):\n",
    "#     if word in w2v.wv.vocab:\n",
    "#         weights.append(w2v.wv[word])\n",
    "#     else:\n",
    "#         weights.append(list(np.random.normal(size=embedding_size)))\n",
    "# weights = torch.tensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(word2ix, open(\"ptt/word2ix\", \"wb\"))\n",
    "# pickle.dump(ix2word, open(\"ptt/ix2word\", \"wb\"))\n",
    "# pickle.dump(qs, open(\"ptt/qs\", \"wb\"))\n",
    "# pickle.dump(anss, open(\"ptt/anss\", \"wb\"))\n",
    "# w2v.save(\"ptt/w2v256.model\")\n",
    "# pickle.dump(weights, open(\"ptt/weights256\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 24 51727\n"
     ]
    }
   ],
   "source": [
    "word2ix = pickle.load(open(\"ptt/word2ix\", \"rb\"))\n",
    "ix2word = pickle.load(open(\"ptt/ix2word\", \"rb\"))\n",
    "qs = pickle.load(open(\"ptt/qs\", \"rb\"))\n",
    "anss = pickle.load(open(\"ptt/anss\", \"rb\"))\n",
    "w2v = Word2Vec.load(\"ptt/w2v256.model\")\n",
    "weights = pickle.load(open(\"ptt/weights256\", \"rb\"))\n",
    "QMAXLEN = max([len(q) for q in qs])\n",
    "AMAXLEN = max([len(a) for a in anss])\n",
    "print(QMAXLEN, AMAXLEN, len(w2v.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTTQADataset(Dataset):\n",
    "    def __init__(self, qs, anss, word2ix):\n",
    "        self.qs = qs\n",
    "        self.anss = anss\n",
    "        self.pairs_ix = self.construct_data(word2ix)\n",
    "    \n",
    "    def construct_data(self, word2ix):\n",
    "        pairs_ix = []\n",
    "        for q, ans in zip(self.qs, self.anss):\n",
    "            q_ix = [word2ix[\"<BOS>\"]] + [word2ix[word] for word in q] + [word2ix[\"<EOS>\"]]\n",
    "            pad_num = QMAXLEN + 2 - len(q_ix)\n",
    "            q_ix += [word2ix[\"<PAD>\"]] * pad_num\n",
    "            ans_ix = [word2ix[\"<BOS>\"]] + [word2ix[word] for word in ans] + [word2ix[\"<EOS>\"]]\n",
    "            pad_num = AMAXLEN + 2 - len(ans_ix)\n",
    "            ans_ix += [word2ix[\"<PAD>\"]] * pad_num\n",
    "            q_ix = self.add_pos(q_ix, QMAXLEN+2)\n",
    "            ans_ix = self.add_pos(ans_ix, AMAXLEN+2)\n",
    "            pairs_ix.append([q_ix, ans_ix])\n",
    "        return np.array(pairs_ix)\n",
    "    \n",
    "    def add_pos(self, seq, l):\n",
    "        seq = np.array(seq).reshape(l, 1)\n",
    "        pos = np.arange(1, l+1).reshape(l, 1)\n",
    "        return np.concatenate((seq, pos), 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.qs)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        pair = [self.qs[ix], self.anss[ix]]\n",
    "        q = torch.LongTensor(self.pairs_ix[ix][0])\n",
    "        ans = torch.LongTensor(self.pairs_ix[ix][1])\n",
    "        return {\"pair\": pair, \"q\": q, \"ans\": ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409838, 8364)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(len(qs))\n",
    "np.random.seed(9987)\n",
    "np.random.shuffle(indices)\n",
    "qs, anss = np.array(qs)[indices], np.array(anss)[indices]\n",
    "val_num = int(0.02 * len(qs))\n",
    "train_qs, train_anss = qs[:-val_num], anss[:-val_num]\n",
    "val_qs, val_anss = qs[-val_num:], anss[-val_num:]\n",
    "qa_set = PTTQADataset(train_qs, train_anss, word2ix)\n",
    "vqa_set = PTTQADataset(val_qs, val_anss, word2ix)\n",
    "len(qa_set), len(vqa_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding_init(n_position, d_pos_vec):\n",
    "    ''' Init the sinusoid position encoding table '''\n",
    "    # keep dim 0 for padding token position encoding zero vector\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
    "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n",
    "def get_attn_padding_mask(seq_q, seq_k):\n",
    "    ''' Indicate the padding-related part to mask '''\n",
    "    assert seq_q.dim() == 2 and seq_k.dim() == 2\n",
    "    mb_size, len_q = seq_q.size()\n",
    "    mb_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(2).unsqueeze(1)   # bx1xsk\n",
    "    pad_attn_mask = pad_attn_mask.expand(mb_size, len_q, len_k) # bxsqxsk\n",
    "    return pad_attn_mask\n",
    "\n",
    "def get_attn_subsequent_mask(seq):\n",
    "    ''' Get an attention mask to avoid using the subsequent info.'''\n",
    "    assert seq.dim() == 2\n",
    "    attn_shape = (seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask)\n",
    "    if seq.is_cuda:\n",
    "        subsequent_mask = subsequent_mask.cuda()\n",
    "    return subsequent_mask\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.eps = 1e-6\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.std(-1, keepdim=True)\n",
    "        output = self.a * (x - mu) / (sigma) + self.b\n",
    "        return output\n",
    "\n",
    "class ScaledDotProductAttn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotProductAttn, self).__init__()\n",
    "        self.scaler = hidden_size ** 0.5\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        # batch, qlen, d_head * batch, d_head, klen\n",
    "        attn_weight = torch.bmm(q, k.transpose(1,2)) / self.scaler\n",
    "        if attn_mask is not None:\n",
    "            attn_weight.data.masked_fill_(attn_mask, -float('inf'))\n",
    "        # batch, qlen, klen\n",
    "        attn_weight = F.softmax(attn_weight, dim=2)\n",
    "        attn_weight = torch.bmm(attn_weight, v)\n",
    "        return attn_weight\n",
    "\n",
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head=8):\n",
    "        super(MultiHeadAttn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.d_head = hidden_size / n_head\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(4)\n",
    "        ])\n",
    "        [init.xavier_normal_(ll.weight) for ll in self.linears]\n",
    "        self.attn = ScaledDotProductAttn(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        batch_size, len_q, _ = q.size()\n",
    "        len_k = k.size(1)\n",
    "        # batch, qlen, emb_size\n",
    "        residual = q\n",
    "        qs, ks, vs = \\\n",
    "            [l(x).view(batch_size, -1, self.n_head, self.d_head).transpose(1, 2) \\\n",
    "             for l, x in zip(self.linears, (q, k, v))]\n",
    "        qs = qs.contiguous().view(-1, len_q, self.d_head)\n",
    "        ks = ks.contiguous().view(-1, len_k, self.d_head)\n",
    "        vs = vs.contiguous().view(-1, len_k, self.d_head)        \n",
    "        # SDP attn: batch*n_head, seqlen, d_head\n",
    "        outputs = self.attn(qs, ks, vs, attn_mask.repeat(self.n_head,1,1))\n",
    "        # concat: batch, seqlen, hidden_size\n",
    "        outputs = outputs.view(-1, self.n_head, len_q, self.d_head).transpose(1,2)\\\n",
    "                        .contiguous().view(-1, len_q, self.n_head*self.d_head)\n",
    "        outputs = self.linears[-1](outputs)\n",
    "        return self.layer_norm(residual + self.dropout(outputs))\n",
    "\n",
    "class PositionWiseDense(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(PositionWiseDense, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size*4)\n",
    "        self.fc2 = nn.Linear(hidden_size*4, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.fc2(F.relu(self.fc1(x)))\n",
    "        return self.layer_norm(residual + self.dropout(output))\n",
    "\n",
    "class EncodeSubLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(EncodeSubLayer, self).__init__()\n",
    "        self.multi_head_attn = MultiHeadAttn(hidden_size)\n",
    "        self.pos_wise_fc = PositionWiseDense(hidden_size)\n",
    "    def forward(self, enc_input, slf_attn_mask=None):\n",
    "        output = self.multi_head_attn(enc_input, enc_input, enc_input, slf_attn_mask)\n",
    "        output = self.pos_wise_fc(output)\n",
    "        return output\n",
    "\n",
    "class DecodeSubLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DecodeSubLayer, self).__init__()\n",
    "        self.multi_head_attn = MultiHeadAttn(hidden_size)\n",
    "        self.enc_attn = MultiHeadAttn(hidden_size)\n",
    "        self.pos_wise_fc = PositionWiseDense(hidden_size)\n",
    "    def forward(self, dec_input, enc_output, slf_attn_mask=None, ende_attn_padding_mask=None):\n",
    "        dec_output = self.multi_head_attn(dec_input, dec_input, dec_input, slf_attn_mask)\n",
    "        dec_output = self.enc_attn(dec_output, enc_output, enc_output, ende_attn_padding_mask)\n",
    "        dec_output = self.pos_wise_fc(dec_output)\n",
    "        return dec_output\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_size, emb_weight, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_pos = QMAXLEN + 3\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pos_enc = nn.Embedding(self.n_pos, embedding_size)\n",
    "        self.pos_enc.weight.data = position_encoding_init(self.n_pos, embedding_size)\n",
    "        self.seq_embedding = nn.Embedding.from_pretrained(emb_weight, freeze=False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncodeSubLayer(hidden_size) for _ in range(6)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src):\n",
    "        seq, pos = src[:,:,0], src[:,:,1]\n",
    "        slf_attn_padding_mask = get_attn_padding_mask(seq, seq)\n",
    "        enc_input = self.seq_embedding(seq)\n",
    "        pos_emb = self.pos_enc(pos)\n",
    "        enc_input += pos_emb\n",
    "        enc_output = self.dropout(enc_input)\n",
    "        for layer in self.layers:\n",
    "            enc_output = layer(enc_output, slf_attn_mask=slf_attn_padding_mask)\n",
    "        return enc_output\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_size, emb_weight, hidden_size, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_pos = AMAXLEN + 2\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pos_enc = nn.Embedding(self.n_pos, embedding_size)\n",
    "        self.pos_enc.weight.data = position_encoding_init(self.n_pos, embedding_size)\n",
    "        self.seq_embedding = nn.Embedding.from_pretrained(emb_weight, freeze=False)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecodeSubLayer(hidden_size) for _ in range(6)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, target, enc_output, src):\n",
    "        seq, pos = target[:,:self.n_pos-1,0], target[:,:self.n_pos-1,1]\n",
    "        \n",
    "        slf_attn_padding_mask = get_attn_padding_mask(seq, seq)\n",
    "        slf_attn_sub_mask = get_attn_subsequent_mask(seq)\n",
    "        slf_attn_mask = torch.gt(slf_attn_padding_mask + slf_attn_sub_mask, 0)\n",
    "        ende_attn_padding_mask = get_attn_padding_mask(seq, src[:,:,0])\n",
    "        \n",
    "        dec_input = self.seq_embedding(seq)\n",
    "        pos_emb = self.pos_enc(pos)\n",
    "        dec_input += pos_emb\n",
    "        dec_output = self.dropout(dec_input)\n",
    "        for layer in self.layers:\n",
    "            dec_output = layer(dec_output, enc_output, slf_attn_mask, ende_attn_padding_mask)\n",
    "        dec_output = self.softmax(self.output_layer(dec_output))\n",
    "        return dec_output\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedding_size, emb_weight, hidden_size, vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(embedding_size, emb_weight, hidden_size)\n",
    "        self.decoder = Decoder(embedding_size, emb_weight, hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, q_seq, ans_seq):\n",
    "        enc_output = self.encoder(q_seq)\n",
    "        dec_output = self.decoder(ans_seq, enc_output, q_seq)\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validating(vdataloader, encoder, decoder, batch_size, file=None):\n",
    "    avg_bleu = []\n",
    "    sf = SmoothingFunction()\n",
    "    for i, data in enumerate(vdataloader):\n",
    "        if i == 6:\n",
    "            avg_bleu = np.mean(avg_bleu)\n",
    "            return avg_bleu\n",
    "        q_seq = data['q'].to(device)\n",
    "        ans_seq = data['ans'].to(device)\n",
    "\n",
    "        enc_output = encoder(q_seq)\n",
    "        for i in range(AMAXLEN-1):\n",
    "            if i == 0:\n",
    "                dec_output = decoder(ans_seq[:,:i+1,:], enc_output, q_seq)\n",
    "            else:\n",
    "                dec_input = torch.cat((ans_seq[:,:1,0], dec_output.max(-1)[-1]), 1).unsqueeze(2)\n",
    "                dec_pos = torch.arange(1, i+2).long().unsqueeze(0).repeat(batch_size, 1).unsqueeze(2).to(device)\n",
    "                dec_input = torch.cat((dec_input, dec_pos), 2)\n",
    "                dec_output = decoder(dec_input, enc_output, q_seq)\n",
    "    #     pdb.set_trace()\n",
    "        pred = dec_output.max(2)[-1]\n",
    "        for j in range(batch_size):\n",
    "            src = [ix2word[wix.item()] for wix in q_seq[j,:,0] if wix.item() != 2]\n",
    "            trt = [ix2word[wix.item()] for wix in ans_seq[j,:,0] if wix.item() != 2]\n",
    "            last_words = []\n",
    "            for wix in pred[j]:\n",
    "                word = ix2word[wix.item()]\n",
    "                last_words.append(word)\n",
    "                if word == \"<EOS>\":\n",
    "                    break\n",
    "                if len(set(last_words[-3:])) == 1 and len(last_words) >= 3:\n",
    "                    last_words = last_words[:-2]\n",
    "                    break\n",
    "            output = last_words\n",
    "            try:\n",
    "                avg_bleu.append(sentence_bleu([trt], output, smoothing_function=sf.method4))\n",
    "            except:\n",
    "                continue\n",
    "            if file is not None:\n",
    "                try:\n",
    "                    file.write(\"\".join(src) + \"\\n\")\n",
    "                    file.write(\"\".join(trt) + \"\\n\")\n",
    "                    file.write(\"\".join(output) + \"\\n\\n\")\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "epochs = 30\n",
    "vocab_size = len(word2ix)\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "iter_size = len(qa_set) // batch_size\n",
    "transformer = Transformer(embedding_size, weights, hidden_size, vocab_size).to(device)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"max\", factor=0.5, patience=3)\n",
    "dataloader = DataLoader(qa_set, batch_size=batch_size, shuffle=True)\n",
    "vdataloader = DataLoader(vqa_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "> <ipython-input-28-8022dd8de08b>(20)<module>()\n",
      "-> epoch_loss += loss.item()\n",
      "(Pdb) loss\n",
      "tensor(11.0784)\n",
      "(Pdb) dec_output.view(-1, vocab_size).size()\n",
      "torch.Size([3200, 51731])\n",
      "(Pdb) dec_output.size()\n",
      "torch.Size([128, 25, 51731])\n",
      "(Pdb) F.cross_entropy(dec_output[:,0,:], ans_seq[:,1,0])\n",
      "tensor(10.8698)\n",
      "(Pdb) F.cross_entropy(dec_output[:,1,:], ans_seq[:,2,0])\n",
      "tensor(11.0534)\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-8022dd8de08b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mans_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0menc_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-8022dd8de08b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mans_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0menc_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start Training\")\n",
    "transformer.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    t1 = time.time()\n",
    "    for i, data in enumerate(dataloader):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        q_seq = data['q'].to(device)\n",
    "        ans_seq = data['ans'].to(device)\n",
    "        \n",
    "        dec_output = transformer(q_seq, ans_seq)\n",
    "\n",
    "        loss = F.nll_loss(dec_output.view(-1, vocab_size), ans_seq[:,1:,0].contiguous().view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    t2 = time.time()\n",
    "    transformer.eval()\n",
    "    bleu = validating(vdataloader, transformer.encoder, transformer.decoder, batch_size)\n",
    "    transformer.train()\n",
    "    scheduler.step(bleu)\n",
    "    epoch_loss /= iter_size\n",
    "    cur_lr = optimizer.param_groups[0]['lr']\n",
    "    print(\"[Epoch %d] Loss: %f, lr: %f, val_bleu: %.3f, Time: %.3f\" % (epoch, epoch_loss, cur_lr, bleu, (t2-t1)))\n",
    "    torch.save(transformer.state_dict(), \"../models/transformer.model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start Testing\")\n",
    "transformer.eval()\n",
    "f = open(\"transformer_result.txt\", \"a\")\n",
    "score = validating(vdataloader, transformer.encoder, transformer.decoder, batch_size, f)\n",
    "f.close()\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
